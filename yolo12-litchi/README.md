# ã€å¤§ä½œä¸š-48ã€‘åŸºäºæ·±åº¦å­¦ä¹ çš„è”æç—…è™«å®³æ£€æµ‹

ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³

è§†é¢‘åœ°å€ï¼š[æ‰‹æŠŠæ‰‹æ•™ä½ ä½¿ç”¨YOLO11å®ç°è½¦è¾†æ£€æµ‹ä¸è¿½è¸ªç³»ç»Ÿ_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1nzzdYwE2g/)

ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³ğŸ¥³

å¤§å®¶å¥½ï¼æˆ‘ä»¬ä»ä»Šå¤©å¼€å§‹åå‘ä¸€ä¸‹å†œä¸šæ–¹å‘çš„ï¼Œå›½å®¶ä¹Ÿæå‡ºæå‡å†œä¸šæ–°è´¨ç”Ÿäº§åŠ› ä»¥ç§‘æŠ€èµ‹èƒ½å¤‡æ˜¥è€•ã€‚æ‰€ä»¥æœ¬æ¬¡æˆ‘ä»¬å¸¦æ¥çš„è‡ªåŠ¨åŒ–ç¨‹åºæ˜¯åŸºäºyolo12å’Œyolo11çš„è”æç—…è™«å®³æ£€æµ‹ï¼Œæœ¬æ¬¡å¸¦æ¥çš„æ£€æµ‹çš„ç±»åˆ«åŒ…å«ä¸‹é¢çš„ç§ç±»ã€‚

* **Anthrax** - ç‚­ç–½ç—…
* **Felt_disease** - æ¯¡ç—…
* **Phytophthora_downy** - è‡´ç—…ç–«éœ‰éœœéœ‰ç—…ï¼ˆâ€œPhytophthoraâ€æ˜¯è‡´ç—…ç–«éœ‰å±ï¼Œå¸¸å¼•èµ·æ¤ç‰©éœœéœ‰ç—…ï¼‰
* **Lychee_Tsubaki_elephant** - è”æå±±èŒ¶è±¡
* **Pedicle_borer** - èŠ±æŸ„èŸ
* **Turtle_backed_beetle** - é¾ŸèƒŒç”²è™«

ä»¥ä¸‹æ˜¯éƒ¨åˆ†æ•°æ®ç¤ºä¾‹ã€‚

![train_batch0](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271501183.jpg)

ä¸‹é¢æ˜¯éƒ¨åˆ†å®ç°æ•ˆæœï¼Œæ”¯æŒè§†é¢‘å’Œå›¾åƒæ£€æµ‹ã€‚

![image-20250327151442223](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271514710.png)

![image-20250327151502687](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271515749.png)

## é¡¹ç›®å®æˆ˜

è¿›è¡Œé¡¹ç›®å®æˆ˜ä¹‹å‰è¯·åŠ¡å¿…å®‰è£…å¥½pytorchå’Œminicondaã€‚

ä¸ä¼šçš„å°ä¼™ä¼´è¯·çœ‹è¿™é‡Œï¼š[Pythoné¡¹ç›®é…ç½®å‰çš„å‡†å¤‡å·¥ä½œ-CSDNåšå®¢](https://blog.csdn.net/ECHOSON/article/details/144233262?sharetype=blogdetail&sharerId=144233262&sharerefer=PC&sharesource=ECHOSON&spm=1011.2480.3001.8118)

<font color='red'>é…ç½®ä¹‹å‰é¦–å…ˆéœ€è¦ä¸‹è½½é¡¹ç›®èµ„æºåŒ…ï¼Œé¡¹ç›®èµ„æºåŒ…è¯·çœ‹ä»ä¸Šæ–¹è§†é¢‘çš„ç½®é¡¶è¯„è®ºä¸­æˆ–è€…æ˜¯åšå®¢ç»‘å®šèµ„æºè·å–å³å¯ã€‚</font>

![image-20250111195350376](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250111195350376.png)

### ç¯å¢ƒé…ç½®

ç¯å¢ƒé…ç½®è¯·çœ‹è¿™é‡Œï¼š[ã€è‚†åäºŒã€‘YOLOç³»åˆ—ä»£ç ç¯å¢ƒé…ç½®ç»Ÿä¸€æµç¨‹-CSDNåšå®¢](https://blog.csdn.net/ECHOSON/article/details/145405669)

### æœ¬åœ°æ¨¡å‹è®­ç»ƒ

æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„è„šæœ¬ä¸º` step1_start_train.py `ï¼Œè¿›è¡Œæ¨¡å‹è®­ç»ƒä¹‹å‰ï¼Œè¯·å…ˆæŒ‰ç…§é…ç½®å¥½ä½ æœ¬åœ°çš„æ•°æ®é›†ã€‚æ•°æ®é›†åœ¨` ultralytics\cfg\datasets\A_my_data.yaml`ç›®å½•ä¸‹ï¼Œä½ éœ€è¦å°†æ•°æ®é›†çš„æ ¹ç›®å½•æ›´æ¢ä¸ºä½ è‡ªå·±æœ¬åœ°çš„ç›®å½•ã€‚

![image-20241204100852481](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241204100852481.png)

![image-20250109222911440](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250109222911440.png)

æ›´æ¢ä¹‹åä¿®æ”¹è®­ç»ƒè„šæœ¬é…ç½®æ–‡ä»¶çš„è·¯å¾„ï¼Œç›´æ¥å³é”®å³å¯å¼€å§‹è®­ç»ƒã€‚

![image-20250109223259429](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250109223259429.png)

è®­ç»ƒå¼€å§‹å‰å¦‚æœå‡ºç°æŠ¥é”™ï¼Œæœ‰å¾ˆå¤§çš„å¯èƒ½æ˜¯æ•°æ®é›†çš„è·¯å¾„æ²¡æœ‰é…ç½®æ­£ç¡®ï¼Œè¯·æ£€æŸ¥æ•°æ®é›†çš„è·¯å¾„ï¼Œä¿è¯æ•°æ®é›†é…ç½®æ²¡æœ‰é—®é¢˜ã€‚è®­ç»ƒä¹‹åçš„ç»“æœå°†ä¼šä¿å­˜åœ¨runsç›®å½•ä¸‹ã€‚

![image-20241204101214326](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241204101214326.png)

### GPUæœåŠ¡å™¨è®­ç»ƒï¼ˆå¯é€‰ï¼‰

ç›®å‰è“è€˜GPUå¯ä»¥è–…ç¾Šæ¯›ï¼Œæ¨èå°ä¼™ä¼´ä»è¿™ä¸ªç½‘ç«™ä½¿ç”¨GPUäº‘æ¥è¿›è¡Œè®­ç»ƒï¼Œæ–°ç”¨æˆ·æ³¨å†Œä¼šè·å¾—30å…ƒçš„ä»£é‡‘åˆ¸ã€‚

æ³¨å†Œåœ°å€ï¼š[è“è€˜GPUæ™ºç®—äº‘å¹³å°](https://cloud.lanyun.net/#/registerPage?promoterCode=0118 )

æœåŠ¡å™¨ä½¿ç”¨æŒ‡å—ï¼š[æ‰‹æŠŠæ‰‹æ•™ä½ ä½¿ç”¨æœåŠ¡å™¨è®­ç»ƒAIæ¨¡å‹_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1TuxLeVED6?vd_source=2f9a4e63109c3db3be5e8078e5111776&spm_id_from=333.788.videopod.sections)

### æ¨¡å‹æµ‹è¯•

æ¨¡å‹çš„æµ‹è¯•ä¸»è¦æ˜¯å¯¹mapã€pã€rç­‰æŒ‡æ ‡è¿›è¡Œè®¡ç®—ï¼Œä½¿ç”¨çš„è„šæœ¬ä¸º` step2_start_val.py`ï¼Œæ¨¡å‹åœ¨è®­ç»ƒçš„æœ€åä¸€è½®å·²ç»æ‰§è¡Œäº†æµ‹è¯•ï¼Œå…¶å®è¿™ä¸ªæ­¥éª¤å®Œå…¨å¯ä»¥è·³è¿‡ï¼Œä½†æ˜¯æœ‰çš„æœ‹å‹å¯èƒ½æƒ³è¦å•ç‹¬éªŒè¯ï¼Œé‚£ä½ åªéœ€è¦æ›´æ”¹æµ‹è¯•è„šæœ¬ä¸­çš„æƒé‡ä¸ºä½ è‡ªå·±æ‰€è®­ç»ƒçš„æƒé‡è·¯å¾„ï¼Œå³å¯å•ç‹¬è¿›è¡Œæµ‹è¯•ã€‚

![image-20241204101429118](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241204101429118.png)

### å›¾å½¢åŒ–ç•Œé¢å°è£…

å›¾å½¢åŒ–ç•Œé¢è¿›è¡Œäº†å‡çº§ï¼Œæœ¬æ¬¡å›¾å½¢åŒ–ç•Œé¢çš„å¼€å‘æˆ‘ä»¬ä½¿ç”¨pyside6æ¥è¿›è¡Œå¼€å‘ã€‚**PySide6** æ˜¯ä¸€ä¸ªå¼€æºçš„Pythonåº“ï¼Œå®ƒæ˜¯Qt 6æ¡†æ¶çš„Pythonç»‘å®šã€‚Qt æ˜¯ä¸€ä¸ªè·¨å¹³å°çš„åº”ç”¨ç¨‹åºå¼€å‘æ¡†æ¶ï¼Œä¸»è¦ç”¨äºå¼€å‘å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰åº”ç”¨ç¨‹åºï¼ŒåŒæ—¶ä¹Ÿæä¾›äº†ä¸°å¯Œçš„åŠŸèƒ½æ¥å¤„ç†éå›¾å½¢åº”ç”¨ç¨‹åºçš„ä»»åŠ¡ï¼ˆå¦‚æ•°æ®åº“ã€ç½‘ç»œç¼–ç¨‹ç­‰ï¼‰ã€‚PySide6 ä½¿å¾—å¼€å‘è€…èƒ½å¤Ÿä½¿ç”¨ Python ç¼–å†™ Qt 6 åº”ç”¨ç¨‹åºï¼Œå› æ­¤ï¼Œå®ƒæä¾›äº†Pythonçš„çµæ´»æ€§å’ŒQt 6çš„å¼ºå¤§åŠŸèƒ½ã€‚å›¾å½¢åŒ–ç•Œé¢æä¾›äº†å›¾ç‰‡å’Œè§†é¢‘æ£€æµ‹ç­‰å¤šä¸ªåŠŸèƒ½ï¼Œå›¾å½¢åŒ–ç•Œé¢çš„ç¨‹åºä¸º` step3_start_window_track.py `ã€‚

å¦‚æœä½ é‡æ–°è®­ç»ƒäº†æ¨¡å‹ï¼Œéœ€è¦æ›¿æ¢ä¸ºä½ è‡ªå·±çš„æ¨¡å‹ï¼Œè¯·åœ¨è¿™é‡Œè¿›è¡Œæ“ä½œã€‚

![image-20241204101842858](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241204101842858.png)

å¦‚æœä½ æƒ³è¦å¯¹å›¾å½¢åŒ–ç•Œé¢çš„é¢˜ç›®ã€logoç­‰è¿›è¡Œä¿®æ”¹ï¼Œç›´æ¥åœ¨è¿™é‡Œä¿®æ”¹å…¨å±€å˜é‡å³å¯ã€‚

![image-20241204101949741](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241204101949741.png)

ç™»å½•ä¹‹åä¸Šä¼ å›¾åƒæˆ–è€…æ˜¯ä¸Šä¼ è§†é¢‘è¿›è¡Œæ£€æµ‹å³å¯ã€‚

![](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271515469.png)

![image-20241211204753525](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241211204753525.png)

å¯¹äºwebç•Œé¢çš„å°è£…ï¼Œå¯¹åº”çš„pythonæ–‡ä»¶æ˜¯`web_demo.py`ï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨gradioæ¥è¿›è¡Œå¼€å‘ï¼Œgradioï¼Œè¯¦ç»†çš„ä»£ç å¦‚ä¸‹ï¼š

```python
#!/usr/bin/env python
# -*- coding: UTF-8 -*-
'''
@Project ï¼šstep3_start_window_track.py 
@File    ï¼šweb_demo.py
@IDE     ï¼šPyCharm 
@Author  ï¼šè‚†åäºŒï¼ˆä»˜è´¹å’¨è¯¢QQ: 3045834499ï¼‰ ç²‰ä¸å¯äº«å—99å…ƒè°ƒè¯•æœåŠ¡
@Description  ï¼šTODO æ·»åŠ æ–‡ä»¶æè¿°
@Date    ï¼š2024/12/11 20:25 
'''
import gradio as gr
import PIL.Image as Image

from ultralytics import ASSETS, YOLO

model = YOLO("runs/yolo11s/weights/best.pt")


def predict_image(img, conf_threshold, iou_threshold):
    """Predicts objects in an image using a YOLO11 model with adjustable confidence and IOU thresholds."""
    results = model.predict(
        source=img,
        conf=conf_threshold,
        iou=iou_threshold,
        show_labels=True,
        show_conf=True,
        imgsz=640,
    )

    for r in results:
        im_array = r.plot()
        im = Image.fromarray(im_array[..., ::-1])

    return im


iface = gr.Interface(
    fn=predict_image,
    inputs=[
        gr.Image(type="pil", label="Upload Image"),
        gr.Slider(minimum=0, maximum=1, value=0.25, label="Confidence threshold"),
        gr.Slider(minimum=0, maximum=1, value=0.45, label="IoU threshold"),
    ],
    outputs=gr.Image(type="pil", label="Result"),
    title="åŸºäºYOLO11çš„åƒåœ¾æ£€æµ‹ç³»ç»Ÿ",
    description="Upload images for inference.",
    # examples=[
    #     [ASSETS / "bus.jpg", 0.25, 0.45],
    #     [ASSETS / "zidane.jpg", 0.25, 0.45],
    # ],
)

if __name__ == "__main__":
    # iface.launch(share=True)
    # iface.launch(share=True)
    iface.launch()
```

## æ–‡æ¡£

### èƒŒæ™¯ä¸æ„ä¹‰

è”æä½œä¸ºæˆ‘å›½åå—åœ°åŒºæœ€é‡è¦çš„ç»æµæ—æœæ ‘ä¹‹ä¸€ï¼Œç§æ¤é¢ç§¯å’Œäº§é‡å‡å±…ä¸–ç•Œé¦–ä½ã€‚ç„¶è€Œï¼Œè”æåœ¨ç§æ¤è¿‡ç¨‹ä¸­å®¹æ˜“å—åˆ°å¤šç§ç—…è™«å®³çš„ä¾µè¢­ï¼Œå¸¸è§çš„ç—…è™«å®³åŒ…æ‹¬è”æç‚­ç–½ç—…ã€è”ææ¯›æ¯¡ç—…ã€è”æå¶ç˜¿èšŠã€è”æè—»æ–‘ç—…å’Œè”æç…¤çƒŸç—…ç­‰ã€‚è¿™äº›ç—…è™«å®³ä¸ä»…ç§ç±»ç¹å¤šï¼Œè€Œä¸”å‘ç—…å‘¨æœŸé•¿ã€é˜²æ²»å›°éš¾ï¼Œä¸¥é‡å½±å“è”æçš„äº§é‡å’Œå“è´¨ã€‚ä¼ ç»Ÿçš„ç—…è™«å®³æ£€æµ‹æ–¹æ³•ä¸»è¦ä¾é äººå·¥æ ¹æ®ç»éªŒç°åœºè§‚å¯Ÿåˆ¤å®šï¼Œå­˜åœ¨åˆ¤å®šæ•ˆç‡ä½ã€æˆæœ¬é«˜ã€å—ä¸»è§‚æ€§å½±å“ç­‰é—®é¢˜ï¼Œæ— æ³•æ»¡è¶³ç°ä»£å†œä¸šå¤§è§„æ¨¡ã€å®æ—¶æ£€æµ‹çš„éœ€æ±‚ã€‚

è¿‘å¹´æ¥ï¼Œéšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäºå›¾åƒè¯†åˆ«çš„ç›®æ ‡æ£€æµ‹æŠ€æœ¯åœ¨å†œä¸šé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚YOLOï¼ˆYou Only Look Onceï¼‰ç³»åˆ—æ¨¡å‹å› å…¶æ£€æµ‹é€Ÿåº¦å¿«ã€å‡†ç¡®ç‡é«˜ï¼Œç‰¹åˆ«é€‚åˆåº”ç”¨äºç§»åŠ¨ç«¯å’ŒåµŒå…¥å¼è®¾å¤‡ã€‚ç„¶è€Œï¼Œä¼ ç»Ÿçš„YOLOæ¨¡å‹åœ¨å¤æ‚èƒŒæ™¯ä¸‹è¡¨ç°ä¸ä½³ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶ç¯å¢ƒä¸‹ï¼ŒèƒŒæ™¯çš„å¤šæ ·æ€§å’Œéç»“æ„åŒ–ç‰¹å¾ä¼šå¯¹æ£€æµ‹ç²¾åº¦äº§ç”Ÿå½±å“ã€‚æ­¤å¤–ï¼Œè”æç—…è™«å®³çš„èŒƒå›´å¤§å°ä¸ä¸€ï¼Œä¹Ÿä¼šå½±å“æ£€æµ‹çš„å‡†ç¡®ç‡ã€‚

ä½¿ç”¨YOLOv12æ¨¡å‹è¿›è¡Œè”æç—…è™«å®³æ£€æµ‹ï¼Œå¯ä»¥å®ç°å¿«é€Ÿã€å‡†ç¡®åœ°è¯†åˆ«ç—…è™«å®³ç±»å‹ï¼Œå¤§å¤§æé«˜äº†æ£€æµ‹æ•ˆç‡ã€‚ä¸ä¼ ç»Ÿçš„äººå·¥æ£€æµ‹æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹èƒ½å¤Ÿå®æ—¶å¤„ç†å¤§é‡å›¾åƒæ•°æ®ï¼Œå‡å°‘äººå·¥å¹²é¢„ï¼Œæé«˜æ£€æµ‹çš„å‡†ç¡®æ€§å’Œå®¢è§‚æ€§ã€‚

æ”¹è¿›åçš„YOLOv12æ¨¡å‹åœ¨å¤šåœºæ™¯ä¸‹è¡¨ç°å‡ºè‰¯å¥½çš„é€‚åº”æ€§å’Œé²æ£’æ€§ã€‚æ— è®ºæ˜¯è‡ªç„¶ç¯å¢ƒè¿˜æ˜¯å®éªŒå®¤ç¯å¢ƒï¼Œè¯¥æ¨¡å‹éƒ½èƒ½æœ‰æ•ˆæ£€æµ‹è”æç—…è™«å®³ï¼Œä¸ºå®é™…åº”ç”¨æä¾›äº†å¯é çš„æŠ€æœ¯æ”¯æŒã€‚

### ç›¸å…³æ–‡çŒ®ç»¼è¿°

éšç€æ·±åº¦å­¦ä¹ æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼ŒåŸºäºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰çš„å›¾åƒè¯†åˆ«æŠ€æœ¯åœ¨å†œä¸šé¢†åŸŸå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ï¼Œå°¤å…¶æ˜¯åœ¨å†œä½œç‰©ç—…è™«å®³æ£€æµ‹æ–¹é¢ã€‚è¿‘å¹´æ¥ï¼ŒYOLOï¼ˆYou Only Look Onceï¼‰ç³»åˆ—æ¨¡å‹å› å…¶æ£€æµ‹é€Ÿåº¦å¿«ã€å‡†ç¡®ç‡é«˜ï¼Œé€æ¸æˆä¸ºå†œä¸šç—…è™«å®³æ£€æµ‹é¢†åŸŸçš„çƒ­é—¨é€‰æ‹©ã€‚
åœ¨è”æç—…è™«å®³æ£€æµ‹æ–¹é¢ï¼Œå·²æœ‰ç ”ç©¶åˆ©ç”¨æ”¹è¿›çš„YOLOæ¨¡å‹å–å¾—äº†æ˜¾è‘—æˆæœã€‚ä¾‹å¦‚ï¼Œæœ‰ç ”ç©¶æå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›YOLO v4çš„è”æç—…è™«å®³æ£€æµ‹æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä½¿ç”¨GhostNetä½œä¸ºä¸»å¹²ç½‘ç»œï¼Œå¹¶ç»“åˆCBAMæ³¨æ„åŠ›æœºåˆ¶ï¼Œæœ‰æ•ˆæé«˜äº†æ£€æµ‹ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†æ¨¡å‹çš„è½»é‡åŒ–ã€‚æ­¤å¤–ï¼Œå¦ä¸€é¡¹ç ”ç©¶é’ˆå¯¹è”æè™«å®³å°ç›®æ ‡æ£€æµ‹é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§åŸºäºæ”¹è¿›YOLOv10nçš„è½»é‡åŒ–æ¨¡å‹YOLO-LPï¼Œé€šè¿‡ä¼˜åŒ–ä¸»å¹²ç½‘ç»œå’Œé¢ˆéƒ¨ç½‘ç»œç»“æ„ï¼Œä»¥åŠå¼•å…¥æ–°çš„æŸå¤±å‡½æ•°ï¼Œæ˜¾è‘—æå‡äº†å°ç›®æ ‡æ£€æµ‹çš„å‡†ç¡®ç‡ã€‚
é™¤äº†æ¨¡å‹æ”¹è¿›ï¼Œæ•°æ®é›†çš„æ„å»ºå’Œæ•°æ®å¢å¼ºæ–¹æ³•ä¹Ÿå¯¹æ£€æµ‹æ•ˆæœèµ·åˆ°äº†é‡è¦ä½œç”¨ã€‚ä¾‹å¦‚ï¼Œåœ¨æ„å»ºè”æç—…è™«å®³å›¾åƒæ•°æ®é›†æ—¶ï¼Œç ”ç©¶äººå‘˜é€šè¿‡åœ¨ä¸åŒå…‰ç…§ã€å¤©æ°”æ¡ä»¶ä¸‹é‡‡é›†å›¾åƒï¼Œå¹¶é‡‡ç”¨å¤šç§æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæé«˜äº†æ¨¡å‹å¯¹å¤æ‚è‡ªç„¶ç¯å¢ƒçš„é€‚åº”æ€§ã€‚æ­¤å¤–ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„è”æè™«å®³è¯†åˆ«æŠ€æœ¯å·²åœ¨å®é™…åœºæ™¯ä¸­å¾—åˆ°åº”ç”¨ï¼Œè¯æ˜äº†å…¶åœ¨çœŸå®å¤æ‚ç¯å¢ƒä¸­è¿›è¡Œæœ‰æ•ˆè¯†åˆ«çš„å¯è¡Œæ€§ã€‚
ç»¼ä¸Šæ‰€è¿°ï¼ŒYOLOç³»åˆ—æ¨¡å‹åœ¨è”æç—…è™«å®³æ£€æµ‹é¢†åŸŸå±•ç°å‡ºäº†è‰¯å¥½çš„åº”ç”¨å‰æ™¯ã€‚æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥æ¢ç´¢æ¨¡å‹çš„ä¼˜åŒ–å’Œæ”¹è¿›ï¼Œä»¥åº”å¯¹æ›´åŠ å¤æ‚çš„è‡ªç„¶ç¯å¢ƒå’Œå¤šæ ·åŒ–çš„ç—…è™«å®³ç±»å‹ã€‚åŒæ—¶ï¼Œç»“åˆå…¶ä»–æ·±åº¦å­¦ä¹ æŠ€æœ¯å’Œæ•°æ®å¢å¼ºæ–¹æ³•ï¼Œæœ‰æœ›è¿›ä¸€æ­¥æé«˜æ£€æµ‹çš„å‡†ç¡®ç‡å’Œæ•ˆç‡ã€‚

### æœ¬æ–‡ç®—æ³•ä»‹ç»

#### yolo12ç®—æ³•ä»‹ç»

![image-20250312171139485](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250312171139485.png)

yolo12ç®—æ³•åœ¨åŸå…ˆyolo11çš„åŸºç¡€ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå¼•å…¥äº†ä¸€ç§ä»¥æ³¨æ„åŠ›ä¸ºä¸­å¿ƒçš„æ¶æ„ï¼Œå®ƒä¸åŒäºä»¥å¾€YOLO æ¨¡å‹ä¸­ä½¿ç”¨çš„åŸºäº CNN çš„ä¼ ç»Ÿæ–¹æ³•ï¼Œä½†ä»ä¿æŒäº†è®¸å¤šåº”ç”¨æ‰€å¿…éœ€çš„å®æ—¶æ¨ç†é€Ÿåº¦ã€‚è¯¥æ¨¡å‹é€šè¿‡å¯¹æ³¨æ„åŠ›æœºåˆ¶å’Œæ•´ä½“ç½‘ç»œæ¶æ„è¿›è¡Œæ–°é¢–çš„æ–¹æ³•åˆ›æ–°ï¼Œå®ç°äº†æœ€å…ˆè¿›çš„ç‰©ä½“æ£€æµ‹ç²¾åº¦ï¼ŒåŒæ—¶ä¿æŒäº†å®æ—¶æ€§èƒ½ã€‚

- **åŒºåŸŸæ³¨æ„æœºåˆ¶**ï¼šä¸€ç§æ–°çš„è‡ªæˆ‘æ³¨æ„æ–¹æ³•ï¼Œèƒ½æœ‰æ•ˆå¤„ç†å¤§çš„æ„Ÿå—é‡ã€‚å®ƒå¯å°†[ç‰¹å¾å›¾](https://www.ultralytics.com/glossary/feature-maps)æ¨ªå‘æˆ–çºµå‘åˆ’åˆ†ä¸º*l ä¸ª*å¤§å°ç›¸ç­‰çš„åŒºåŸŸï¼ˆé»˜è®¤ä¸º 4 ä¸ªï¼‰ï¼Œä»è€Œé¿å…å¤æ‚çš„æ“ä½œï¼Œå¹¶ä¿æŒè¾ƒå¤§çš„æœ‰æ•ˆæ„Ÿå—é‡ã€‚ä¸æ ‡å‡†è‡ªæ³¨æ„ç›¸æ¯”ï¼Œè¿™å¤§å¤§é™ä½äº†è®¡ç®—æˆæœ¬ã€‚
- **å‰©ä½™æ•ˆç‡å±‚èšåˆç½‘ç»œï¼ˆR-ELANï¼‰:**åŸºäº ELAN çš„æ”¹è¿›å‹ç‰¹å¾èšåˆæ¨¡å—ï¼Œæ—¨åœ¨è§£å†³ä¼˜åŒ–éš¾é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨ä»¥æ³¨æ„åŠ›ä¸ºä¸­å¿ƒçš„å¤§è§„æ¨¡æ¨¡å‹ä¸­ã€‚R-ELAN å¼•å…¥äº†
  - å…·æœ‰ç¼©æ”¾åŠŸèƒ½çš„å—çº§æ®‹å·®è¿æ¥ï¼ˆç±»ä¼¼äºå›¾å±‚ç¼©æ”¾ï¼‰ã€‚
  - é‡æ–°è®¾è®¡çš„ç‰¹å¾èšåˆæ–¹æ³•å¯åˆ›å»ºç±»ä¼¼ç“¶é¢ˆçš„ç»“æ„ã€‚
- **ä¼˜åŒ–æ³¨æ„åŠ›æ¶æ„:** YOLO12 ç®€åŒ–äº†æ ‡å‡†å…³æ³¨æœºåˆ¶ï¼Œä»¥æé«˜æ•ˆç‡å¹¶ä¸YOLO æ¡†æ¶å…¼å®¹ã€‚è¿™åŒ…æ‹¬
  - ä½¿ç”¨ FlashAttention å°½é‡å‡å°‘å†…å­˜è®¿é—®å¼€é”€ã€‚
  - å»é™¤ä½ç½®ç¼–ç ï¼Œä½¿æ¨¡å‹æ›´ç®€æ´ã€æ›´å¿«é€Ÿã€‚
  - è°ƒæ•´ MLP æ¯”ä¾‹ï¼ˆä»é€šå¸¸çš„ 4 è°ƒæ•´ä¸º 1.2 æˆ– 2ï¼‰ï¼Œä»¥æ›´å¥½åœ°å¹³è¡¡æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆå±‚ä¹‹é—´çš„è®¡ç®—ã€‚
  - å‡å°‘å †å åŒºå—çš„æ·±åº¦ï¼Œæé«˜ä¼˜åŒ–æ•ˆæœã€‚
  - é…Œæƒ…åˆ©ç”¨å·ç§¯è¿ç®—ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚
  - åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­åŠ å…¥ 7x7 å¯åˆ†ç¦»å·ç§¯ï¼ˆ"ä½ç½®æ„ŸçŸ¥å™¨"ï¼‰ï¼Œå¯¹ä½ç½®ä¿¡æ¯è¿›è¡Œéšå¼ç¼–ç ã€‚

åœ¨é…ç½®æ–‡ä»¶ä¸Šï¼Œä»–å’Œyolo11çš„åŒºåˆ«å°±æ¯”è¾ƒå°äº†ï¼Œä¸»è¦ä½“ç°åœ¨ä¸‹å›¾çº¢æ¡†ä¸­æ‰€ç¤ºçš„åŒºåŸŸã€‚

![image-20250312170003994](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250312170003994.png)

å…¶ä¸­A2C2Få°±æ˜¯yolo12ä¸­æ‰€æå‡ºçš„ä¸»è¦æ¨¡å—ï¼ŒAè¡¨ç¤ºçš„å«ä¹‰æ˜¯Area Attentionã€‚ä¸ºäº†å…‹æœä¼ ç»Ÿè‡ªæ³¨æ„åŠ›æœºåˆ¶è®¡ç®—å¤æ‚åº¦é«˜çš„é—®é¢˜ï¼ŒYOLOv12é€šè¿‡åˆ›æ–°çš„åŒºåŸŸæ³¨æ„åŠ›æ¨¡å—ï¼ˆArea Attentionï¼ŒA2ï¼‰ï¼Œåˆ†è¾¨ç‡ä¸º(H, W)çš„ç‰¹å¾å›¾è¢«åˆ’åˆ†ä¸ºlä¸ªå¤§å°ä¸º(H/l, W)æˆ–(H, W/l)çš„æ®µã€‚è¿™æ¶ˆé™¤äº†æ˜¾å¼çš„çª—å£åˆ’åˆ†ï¼Œä»…éœ€è¦ç®€å•çš„é‡å¡‘æ“ä½œï¼Œä»è€Œå®ç°æ›´å¿«çš„é€Ÿåº¦ã€‚å°†lçš„é»˜è®¤å€¼è®¾ç½®ä¸º4ï¼Œå°†æ„Ÿå—é‡å‡å°åˆ°åŸæ¥çš„1/4ï¼Œä½†ä»ä¿æŒè¾ƒå¤§çš„æ„Ÿå—é‡ã€‚é‡‡ç”¨è¿™ç§æ–¹æ³•ï¼Œæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æˆæœ¬ä»2nÂ²hdé™ä½åˆ°1/2nÂ²hdã€‚å°½ç®¡å­˜åœ¨nÂ²çš„å¤æ‚åº¦ï¼Œä½†å½“nå›ºå®šä¸º640æ—¶ï¼ˆå¦‚æœè¾“å…¥åˆ†è¾¨ç‡å¢åŠ ï¼Œåˆ™nä¼šå¢åŠ ï¼‰ï¼Œè¿™ä»ç„¶è¶³å¤Ÿé«˜æ•ˆï¼Œå¯ä»¥æ»¡è¶³YOLOç³»ç»Ÿçš„å®æ—¶è¦æ±‚ã€‚A2é™ä½äº†æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—æˆæœ¬ï¼ŒåŒæ—¶ä¿æŒè¾ƒå¤§çš„æ„Ÿå—é‡ï¼Œæ˜¾è‘—æå‡äº†æ£€æµ‹ç²¾åº¦ã€‚å¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå³ä¾§çš„åŒºåŸŸæ‰€è¡¨ç¤ºçš„å°±æ˜¯ä½œè€…æå‡ºçš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç›¸å½“äºæ˜¯ä»¥è¾ƒå°‘çš„è®¡ç®—é‡å°±æ•æ‰åˆ°äº†ç›¸å…³çš„åŒºåŸŸã€‚

![image-20250312170135607](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250312170135607.png)

ä¸‹é¢æ˜¯Area Attentionçš„å®ç°ï¼Œå¦‚ä¸‹ã€‚

```python
class A2C2f(nn.Module):
    """
    Area-Attention C2f module for enhanced feature extraction with area-based attention mechanisms.

    This module extends the C2f architecture by incorporating area-attention and ABlock layers for improved feature
    processing. It supports both area-attention and standard convolution modes.

    Attributes:
        cv1 (Conv): Initial 1x1 convolution layer that reduces input channels to hidden channels.
        cv2 (Conv): Final 1x1 convolution layer that processes concatenated features.
        gamma (nn.Parameter | None): Learnable parameter for residual scaling when using area attention.
        m (nn.ModuleList): List of either ABlock or C3k modules for feature processing.

    Methods:
        forward: Processes input through area-attention or standard convolution pathway.

    Examples:
        >>> m = A2C2f(512, 512, n=1, a2=True, area=1)
        >>> x = torch.randn(1, 512, 32, 32)
        >>> output = m(x)
        >>> print(output.shape)
        torch.Size([1, 512, 32, 32])
    """
    def __init__(self, c1, c2, n=1, a2=True, area=1, residual=False, mlp_ratio=2.0, e=0.5, g=1, shortcut=True):
        """
        Area-Attention C2f module for enhanced feature extraction with area-based attention mechanisms.

        Args:
            c1 (int): Number of input channels.
            c2 (int): Number of output channels.
            n (int): Number of ABlock or C3k modules to stack.
            a2 (bool): Whether to use area attention blocks. If False, uses C3k blocks instead.
            area (int): Number of areas the feature map is divided.
            residual (bool): Whether to use residual connections with learnable gamma parameter.
            mlp_ratio (float): Expansion ratio for MLP hidden dimension.
            e (float): Channel expansion ratio for hidden channels.
            g (int): Number of groups for grouped convolutions.
            shortcut (bool): Whether to use shortcut connections in C3k blocks.
        """
        super().__init__()
        c_ = int(c2 * e)  # hidden channels
        assert c_ % 32 == 0, "Dimension of ABlock be a multiple of 32."

        self.cv1 = Conv(c1, c_, 1, 1)
        self.cv2 = Conv((1 + n) * c_, c2, 1)

        self.gamma = nn.Parameter(0.01 * torch.ones(c2), requires_grad=True) if a2 and residual else None
        self.m = nn.ModuleList(
            nn.Sequential(*(ABlock(c_, c_ // 32, mlp_ratio, area) for _ in range(2)))
            if a2
            else C3k(c_, c_, 2, shortcut, g)
            for _ in range(n)
        )

    def forward(self, x):
        """Forward pass through R-ELAN layer."""
        y = [self.cv1(x)]
        y.extend(m(y[-1]) for m in self.m)
        y = self.cv2(torch.cat(y, 1))
        if self.gamma is not None:
            return x + self.gamma.view(-1, len(self.gamma), 1, 1) * y
        return y
```

#### yolo11ç®—æ³•ä»‹ç»

yoloç³»åˆ—å·²ç»åœ¨ä¸šç•Œå¯è°“æ˜¯å®¶å–»æˆ·æ™“äº†ï¼Œä¸‹é¢æ˜¯yolo11æ”¾å‡ºçš„æ€§èƒ½æµ‹è¯•å›¾ï¼Œå…¶ä¸­è¿™ç§å›¾çš„æ¨ªè½´ä¸ºæ¨¡å‹çš„é€Ÿåº¦ï¼Œä¸€èˆ¬æƒ…å†µä¸‹æ¨¡å‹çš„é€Ÿåº¦æ˜¯é€šè¿‡è°ƒæ•´å·ç§¯çš„æ·±åº¦å’Œå®½åº¦æ¥è¿›è¡Œä¿®æ”¹çš„ï¼Œçºµè½´åˆ™è¡¨ç¤ºæ¨¡å‹çš„ç²¾åº¦ï¼Œå¯ä»¥çœ‹åˆ°åœ¨åŒæ ·çš„é€Ÿåº¦ä¸‹ï¼Œ11è¡¨ç°å‡ºæ›´é«˜çš„ç²¾åº¦ã€‚

![image-20241024170914031](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241024170914031.png)

YOLOæ¶æ„çš„æ ¸å¿ƒç”±ä¸‰ä¸ªåŸºæœ¬ç»„ä»¶ç»„æˆã€‚é¦–å…ˆï¼Œä¸»å¹²ä½œä¸ºä¸»è¦ç‰¹å¾æå–å™¨ï¼Œåˆ©ç”¨å·ç§¯ç¥ç»ç½‘ç»œå°†åŸå§‹å›¾åƒæ•°æ®è½¬æ¢æˆå¤šå°ºåº¦ç‰¹å¾å›¾ã€‚å…¶æ¬¡ï¼Œé¢ˆéƒ¨ç»„ä»¶ä½œä¸ºä¸­é—´å¤„ç†é˜¶æ®µï¼Œä½¿ç”¨ä¸“é—¨çš„å±‚æ¥èšåˆå’Œå¢å¼ºä¸åŒå°ºåº¦çš„ç‰¹å¾è¡¨ç¤ºã€‚ç¬¬ä¸‰ï¼Œå¤´éƒ¨åˆ†é‡ä½œä¸ºé¢„æµ‹æœºåˆ¶ï¼Œæ ¹æ®ç²¾ç»†åŒ–çš„ç‰¹å¾æ˜ å°„ç”Ÿæˆç›®æ ‡å®šä½å’Œåˆ†ç±»çš„æœ€ç»ˆè¾“å‡ºã€‚åŸºäºè¿™ä¸ªå·²å»ºç«‹çš„ä½“ç³»ç»“æ„ï¼ŒYOLO11æ‰©å±•å¹¶å¢å¼ºäº†YOLOv8å¥ å®šçš„åŸºç¡€ï¼Œå¼•å…¥äº†ä½“ç³»ç»“æ„åˆ›æ–°å’Œå‚æ•°ä¼˜åŒ–ï¼Œä»¥å®ç°å¦‚å›¾1æ‰€ç¤ºçš„å“è¶Šæ£€æµ‹æ€§èƒ½ã€‚ä¸‹é¢æ˜¯yolo11æ¨¡å‹æ‰€èƒ½æ”¯æŒçš„ä»»åŠ¡ï¼Œç›®æ ‡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²ã€ç‰©ä½“åˆ†ç±»ã€å§¿æ€ä¼°è®¡ã€æ—‹è½¬ç›®æ ‡æ£€æµ‹å’Œç›®æ ‡è¿½è¸ªä»–éƒ½å¯ä»¥ï¼Œå¦‚æœä½ æƒ³è¦é€‰æ‹©ä¸€ä¸ªæ·±åº¦å­¦ä¹ ç®—æ³•æ¥è¿›è¡Œå…¥é—¨ï¼Œé‚£ä¹ˆyolo11å°†ä¼šæ˜¯ä½ ç»ä½³çš„é€‰æ‹©ã€‚

![image-20241024171109729](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241024171109729.png)

ä¸ºäº†èƒ½å¤Ÿè®©å¤§å®¶å¯¹yolo11ç½‘ç»œæœ‰æ¯”è¾ƒæ¸…æ™°çš„ç†è§£ï¼Œä¸‹é¢æˆ‘å°†ä¼šå¯¹yolo11çš„ç»“æ„è¿›è¡Œæ‹†è§£ã€‚

é¦–å…ˆæ˜¯yolo11çš„ç½‘ç»œç»“æ„æ•´ä½“é¢„è§ˆï¼Œå…¶ä¸­backboneçš„éƒ¨åˆ†ä¸»è¦è´Ÿè´£åŸºç¡€çš„ç‰¹å¾æå–ã€neckçš„éƒ¨åˆ†è´Ÿè´£ç‰¹å¾çš„èåˆï¼Œheadçš„éƒ¨åˆ†è´Ÿè´£è§£ç ï¼Œè®©ä½ çš„ç½‘ç»œå¯ä»¥é€‚é…ä¸åŒçš„è®¡ç®—æœºè§†è§‰çš„ä»»åŠ¡ã€‚

![image-20241024173654996](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241024173654996.png)

* ä¸»å¹²ç½‘ç»œï¼ˆBackBoneï¼‰

  * Conv

    å·ç§¯æ¨¡å—æ˜¯ä¸€ä¸ªå¸¸è§„çš„å·ç§¯æ¨¡å—ï¼Œåœ¨yoloä¸­ä½¿ç”¨çš„éå¸¸å¤šï¼Œå¯ä»¥è®¾è®¡å·ç§¯çš„å¤§å°å’Œæ­¥é•¿ï¼Œä»£ç çš„è¯¦ç»†å®ç°å¦‚ä¸‹ï¼š

    ```python
    class Conv(nn.Module):
        """Standard convolution with args(ch_in, ch_out, kernel, stride, padding, groups, dilation, activation)."""
    
        default_act = nn.SiLU()  # default activation
    
        def __init__(self, c1, c2, k=1, s=1, p=None, g=1, d=1, act=True):
            """Initialize Conv layer with given arguments including activation."""
            super().__init__()
            self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p, d), groups=g, dilation=d, bias=False)
            self.bn = nn.BatchNorm2d(c2)
            self.act = self.default_act if act is True else act if isinstance(act, nn.Module) else nn.Identity()
    
        def forward(self, x):
            """Apply convolution, batch normalization and activation to input tensor."""
            return self.act(self.bn(self.conv(x)))
    
        def forward_fuse(self, x):
            """Perform transposed convolution of 2D data."""
            return self.act(self.conv(x))
    ```

  * C3k2

    C3k2å—è¢«æ”¾ç½®åœ¨å¤´éƒ¨çš„å‡ ä¸ªé€šé“ä¸­ï¼Œç”¨äºå¤„ç†ä¸åŒæ·±åº¦çš„å¤šå°ºåº¦ç‰¹å¾ã€‚ä»–çš„ä¼˜åŠ¿æœ‰ä¸¤ä¸ªæ–¹é¢ã€‚ä¸€ä¸ªæ–¹é¢æ˜¯è¿™ä¸ªæ¨¡å—æä¾›äº†æ›´å¿«çš„å¤„ç†:ä¸å•ä¸ªå¤§å·ç§¯ç›¸æ¯”ï¼Œä½¿ç”¨ä¸¤ä¸ªè¾ƒå°çš„å·ç§¯å¯ä»¥å‡å°‘è®¡ç®—å¼€é”€ï¼Œä»è€Œæ›´å¿«åœ°æå–ç‰¹å¾ã€‚å¦ä¸€ä¸ªæ–¹é¢æ˜¯è¿™ä¸ªæ¨¡å—æä¾›äº†æ›´å¥½çš„å‚æ•°æ•ˆç‡: C3k2æ˜¯CSPç“¶é¢ˆçš„ä¸€ä¸ªæ›´ç´§å‡‘çš„ç‰ˆæœ¬ï¼Œä½¿æ¶æ„åœ¨å¯è®­ç»ƒå‚æ•°çš„æ•°é‡æ–¹é¢æ›´é«˜æ•ˆã€‚

    C3k2æ¨¡å—ä¸»è¦æ˜¯ä¸ºäº†å¢åŠ ç‰¹å¾çš„å¤šæ ·æ€§ï¼Œå…¶ä¸­è¿™å—æ¨¡å—æ˜¯ç”±C3kæ¨¡å—æ¼”å˜è€Œæ¥ã€‚å®ƒé€šè¿‡å…è®¸è‡ªå®šä¹‰å†…æ ¸å¤§å°æä¾›äº†å¢å¼ºçš„çµæ´»æ€§ã€‚C3kçš„é€‚åº”æ€§å¯¹äºä»å›¾åƒä¸­æå–æ›´è¯¦ç»†çš„ç‰¹å¾ç‰¹åˆ«æœ‰ç”¨ï¼Œæœ‰åŠ©äºæé«˜æ£€æµ‹ç²¾åº¦ã€‚C3kçš„å®ç°å¦‚ä¸‹ã€‚

    ```python
    class C3k(C3):
        """C3k is a CSP bottleneck module with customizable kernel sizes for feature extraction in neural networks."""
    
        def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5, k=3):
            """Initializes the C3k module with specified channels, number of layers, and configurations."""
            super().__init__(c1, c2, n, shortcut, g, e)
            c_ = int(c2 * e)  # hidden channels
            # self.m = nn.Sequential(*(RepBottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))
            self.m = nn.Sequential(*(Bottleneck(c_, c_, shortcut, g, k=(k, k), e=1.0) for _ in range(n)))
    ```

    å¦‚æœå°†c3kä¸­çš„nè®¾ç½®ä¸º2ï¼Œåˆ™æ­¤æ—¶çš„æ¨¡å—å³ä¸ºC3K2æ¨¡å—ï¼Œç½‘ç»œç»“æ„å›¾å¦‚ä¸‹æ‰€ç¤ºã€‚

    ![image-20241025121912923](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241025121912923.png)

    è¯¥ç½‘ç»œçš„å®ç°ä»£ç å¦‚ä¸‹ã€‚

    ```python
    class C3k2(C2f):
        """Faster Implementation of CSP Bottleneck with 2 convolutions."""
    
        def __init__(self, c1, c2, n=1, c3k=False, e=0.5, g=1, shortcut=True):
            """Initializes the C3k2 module, a faster CSP Bottleneck with 2 convolutions and optional C3k blocks."""
            super().__init__(c1, c2, n, shortcut, g, e)
            self.m = nn.ModuleList(
                C3k(self.c, self.c, 2, shortcut, g) if c3k else Bottleneck(self.c, self.c, shortcut, g) for _ in range(n)
            )
    ```

  * C2PSA

    PSAçš„æ¨¡å—èµ·åˆåœ¨YOLOv10ä¸­æå‡ºï¼Œé€šè¿‡è‡ªæ³¨æ„åŠ›çš„æœºåˆ¶å¢åŠ ç‰¹å¾çš„è¡¨è¾¾èƒ½åŠ›ï¼Œç›¸å¯¹äºä¼ ç»Ÿçš„è‡ªæ³¨æ„åŠ›æœºåˆ¶è€Œè¨€ï¼Œè®¡ç®—é‡åˆç›¸å¯¹è¾ƒå°ã€‚ç½‘ç»œçš„ç»“æ„å›¾å¦‚ä¸‹æ‰€ç¤ºï¼Œå…¶ä¸­å›¾ä¸­çš„mhsaè¡¨ç¤ºçš„æ˜¯å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ŒFFNè¡¨ç¤ºå‰é¦ˆç¥ç»ç½‘ç»œã€‚

    ![image-20241025122617233](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241025122617233.png)

    

  åœ¨è¿™ä¸ªåŸºç¡€ä¸Šæ·»åŠ ç»™åŸå…ˆçš„C2æ¨¡å—ä¸Šæ·»åŠ ä¸€ä¸ªPSAçš„æ—è·¯åˆ™æ„æˆäº†C2PSAçš„æ¨¡å—ï¼Œè¯¥æ¨¡å—çš„ç¤ºæ„å›¾å¦‚ä¸‹ã€‚

  ![image-20241025122752167](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241025122752167.png)

  ç½‘ç»œå®ç°å¦‚ä¸‹ï¼š

  ```python
  class C2PSA(nn.Module):
      """
      C2PSA module with attention mechanism for enhanced feature extraction and processing.
  
      This module implements a convolutional block with attention mechanisms to enhance feature extraction and processing
      capabilities. It includes a series of PSABlock modules for self-attention and feed-forward operations.
  
      Attributes:
          c (int): Number of hidden channels.
          cv1 (Conv): 1x1 convolution layer to reduce the number of input channels to 2*c.
          cv2 (Conv): 1x1 convolution layer to reduce the number of output channels to c.
          m (nn.Sequential): Sequential container of PSABlock modules for attention and feed-forward operations.
  
      Methods:
          forward: Performs a forward pass through the C2PSA module, applying attention and feed-forward operations.
  
      Notes:
          This module essentially is the same as PSA module, but refactored to allow stacking more PSABlock modules.
  
      Examples:
          >>> c2psa = C2PSA(c1=256, c2=256, n=3, e=0.5)
          >>> input_tensor = torch.randn(1, 256, 64, 64)
          >>> output_tensor = c2psa(input_tensor)
      """
  
      def __init__(self, c1, c2, n=1, e=0.5):
          """Initializes the C2PSA module with specified input/output channels, number of layers, and expansion ratio."""
          super().__init__()
          assert c1 == c2
          self.c = int(c1 * e)
          self.cv1 = Conv(c1, 2 * self.c, 1, 1)
          self.cv2 = Conv(2 * self.c, c1, 1)
  
          self.m = nn.Sequential(*(PSABlock(self.c, attn_ratio=0.5, num_heads=self.c // 64) for _ in range(n)))
  
      def forward(self, x):
          """Processes the input tensor 'x' through a series of PSA blocks and returns the transformed tensor."""
          a, b = self.cv1(x).split((self.c, self.c), dim=1)
          b = self.m(b)
          return self.cv2(torch.cat((a, b), 1))
  
  ```

* é¢ˆéƒ¨ç½‘ç»œï¼ˆNeckï¼‰

  * upsample

    è¿™é‡Œæ˜¯ä¸€ä¸ªå¸¸ç”¨çš„ä¸Šé‡‡æ ·çš„æ–¹å¼ï¼Œåœ¨YOLO11çš„æ¨¡å‹ä¸­ï¼Œè¿™é‡Œä¸€èˆ¬ä½¿ç”¨æœ€è¿‘é‚»å·®å€¼çš„æ–¹å¼æ¥è¿›è¡Œå®ç°ã€‚åœ¨ `torch`ï¼ˆPyTorchï¼‰ä¸­ï¼Œ`upsample` æ“ä½œæ˜¯ç”¨äºå¯¹å¼ é‡ï¼ˆé€šå¸¸æ˜¯å›¾åƒæˆ–ç‰¹å¾å›¾ï¼‰è¿›è¡Œ**ä¸Šé‡‡æ ·**ï¼ˆå¢å¤§å°ºå¯¸ï¼‰çš„æ“ä½œã€‚ä¸Šé‡‡æ ·çš„ä¸»è¦ç›®çš„æ˜¯å¢åŠ ç‰¹å¾å›¾çš„ç©ºé—´åˆ†è¾¨ç‡ï¼Œåœ¨æ·±åº¦å­¦ä¹ ä¸­é€šå¸¸ç”¨äº**å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰**ä¸­ç”Ÿæˆé«˜åˆ†è¾¨ç‡çš„ç‰¹å¾å›¾ï¼Œç‰¹åˆ«æ˜¯åœ¨ä»»åŠ¡å¦‚ç›®æ ‡æ£€æµ‹ã€è¯­ä¹‰åˆ†å‰²å’Œç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰ä¸­ã€‚

    PyTorch ä¸­çš„ `torch.nn.functional.upsample` åœ¨è¾ƒæ—©ç‰ˆæœ¬æä¾›äº†ä¸Šé‡‡æ ·åŠŸèƒ½ï¼Œä½†åœ¨æ–°çš„ç‰ˆæœ¬ä¸­æ¨èä½¿ç”¨ `torch.nn.functional.interpolate`ï¼ŒåŠŸèƒ½ç›¸åŒï¼Œä½†æ›´åŠ çµæ´»å’Œæ ‡å‡†åŒ–ã€‚

    ä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

    `torch.nn.functional.interpolate` å‡½æ•°ç”¨äºä¸Šé‡‡æ ·ï¼Œæ”¯æŒä¸åŒçš„æ’å€¼æ–¹æ³•ï¼Œå¸¸ç”¨çš„å‚æ•°å¦‚ä¸‹ï¼š

    ```python
    torch.nn.functional.interpolate(input, size=None, scale_factor=None, mode='nearest', align_corners=None)
    ```

    - `input`ï¼šè¾“å…¥çš„å¼ é‡ï¼Œé€šå¸¸æ˜¯ 4D çš„å¼ é‡ï¼Œå½¢çŠ¶ä¸º `(batch_size, channels, height, width)`ã€‚

    - `size`ï¼šè¾“å‡ºçš„ç›®æ ‡å°ºå¯¸ï¼Œå¯ä»¥æ˜¯æ•´å‹çš„é«˜åº¦å’Œå®½åº¦ï¼ˆå¦‚ `(height, width)`ï¼‰ï¼Œè¡¨ç¤ºå¸Œæœ›å°†ç‰¹å¾å›¾è°ƒæ•´åˆ°çš„å…·ä½“å°ºå¯¸ã€‚

    - `scale_factor`ï¼šä¸Šé‡‡æ ·çš„ç¼©æ”¾å› å­ã€‚ä¾‹å¦‚ï¼Œ`scale_factor=2` è¡¨ç¤ºç‰¹å¾å›¾çš„é«˜åº¦å’Œå®½åº¦éƒ½æ‰©å¤§ 2 å€ã€‚å¦‚æœè®¾ç½®äº† `scale_factor`ï¼Œåˆ™ä¸éœ€è¦å†è®¾ç½® `size`ã€‚

    - ```
      mode
      ```

      ï¼šæ’å€¼çš„æ–¹å¼ï¼Œæœ‰å¤šç§å¯é€‰æ’å€¼ç®—æ³•ï¼š

      - `'nearest'`ï¼šæœ€è¿‘é‚»æ’å€¼ï¼ˆé»˜è®¤ï¼‰ã€‚ç›´æ¥å¤åˆ¶æœ€è¿‘çš„åƒç´ å€¼ï¼Œè®¡ç®—ç®€å•ï¼Œé€Ÿåº¦å¿«ï¼Œä½†ç”Ÿæˆå›¾åƒå¯èƒ½æ¯”è¾ƒç²—ç³™ã€‚
      - `'linear'`ï¼šåŒçº¿æ€§æ’å€¼ï¼Œé€‚ç”¨äº 3D è¾“å…¥ï¼ˆå³ 1D ç‰¹å¾å›¾ï¼‰ã€‚
      - `'bilinear'`ï¼šåŒçº¿æ€§æ’å€¼ï¼Œé€‚ç”¨äº 4D è¾“å…¥ï¼ˆå³ 2D ç‰¹å¾å›¾ï¼‰ã€‚
      - `'trilinear'`ï¼šä¸‰çº¿æ€§æ’å€¼ï¼Œé€‚ç”¨äº 5D è¾“å…¥ï¼ˆå³ 3D ç‰¹å¾å›¾ï¼‰ã€‚
      - `'bicubic'`ï¼šåŒä¸‰æ¬¡æ’å€¼ï¼Œè®¡ç®—æ›´å¤æ‚ï¼Œä½†ç”Ÿæˆçš„å›¾åƒæ›´å¹³æ»‘ã€‚

    - `align_corners`ï¼šåœ¨ä½¿ç”¨åŒçº¿æ€§ã€ä¸‰çº¿æ€§ç­‰æ’å€¼æ—¶å†³å®šæ˜¯å¦å¯¹é½è§’ç‚¹ã€‚å¦‚æœä¸º `True`ï¼Œè¾“å…¥å’Œè¾“å‡ºç‰¹å¾å›¾çš„è§’ç‚¹ä¼šå¯¹é½ï¼Œé€šå¸¸ä¼šä½¿æ’å€¼ç»“æœæ›´åŠ ç²¾ç¡®ã€‚

  * Concat

    åœ¨YOLOï¼ˆYou Only Look Onceï¼‰ç›®æ ‡æ£€æµ‹ç½‘ç»œä¸­ï¼Œ`concat`ï¼ˆè¿æ¥ï¼‰æ“ä½œæ˜¯ç”¨äºå°†æ¥è‡ªä¸åŒå±‚çš„ç‰¹å¾å›¾æ‹¼æ¥èµ·æ¥çš„æ“ä½œã€‚å…¶ä½œç”¨æ˜¯èåˆä¸åŒå°ºåº¦çš„ç‰¹å¾ä¿¡æ¯ï¼Œä»¥ä¾¿ç½‘ç»œèƒ½å¤Ÿåœ¨å¤šä¸ªå°ºåº¦ä¸Šæ›´å¥½åœ°è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚è°ƒæ•´å¥½å°ºå¯¸åï¼Œæ²¿ç€**é€šé“ç»´åº¦**å°†ç‰¹å¾å›¾è¿›è¡Œæ‹¼æ¥ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªç‰¹å¾å›¾ï¼Œåˆ†åˆ«å…·æœ‰å½¢çŠ¶ (H, W, C1) å’Œ (H, W, C2)ï¼Œæ‹¼æ¥åå¾—åˆ°çš„ç‰¹å¾å›¾å½¢çŠ¶å°†æ˜¯ (H, W, C1+C2)ï¼Œå³é€šé“æ•°å¢åŠ äº†ã€‚ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œåœ¨è¿›è¡Œconcatæ“ä½œä¹‹åä¼šå†è¿›è¡Œä¸€æ¬¡å·ç§¯çš„æ“ä½œï¼Œé€šè¿‡å·ç§¯çš„æ“ä½œå¯ä»¥å°†é€šé“æ•°è°ƒæ•´åˆ°ç†æƒ³çš„å¤§å°ã€‚è¯¥æ“ä½œçš„å®ç°å¦‚ä¸‹ã€‚

    ```python
    class Concat(nn.Module):
        """Concatenate a list of tensors along dimension."""
    
        def __init__(self, dimension=1):
            """Concatenates a list of tensors along a specified dimension."""
            super().__init__()
            self.d = dimension
    
        def forward(self, x):
            """Forward pass for the YOLOv8 mask Proto module."""
            return torch.cat(x, self.d)
    ```

* å¤´éƒ¨ï¼ˆHeadï¼‰

  YOLOv11çš„Headè´Ÿè´£ç”Ÿæˆç›®æ ‡æ£€æµ‹å’Œåˆ†ç±»æ–¹é¢çš„æœ€ç»ˆé¢„æµ‹ã€‚å®ƒå¤„ç†ä»é¢ˆéƒ¨ä¼ é€’çš„ç‰¹å¾æ˜ å°„ï¼Œæœ€ç»ˆè¾“å‡ºå›¾åƒå†…å¯¹è±¡çš„è¾¹ç•Œæ¡†å’Œç±»æ ‡ç­¾ã€‚ä¸€èˆ¬è´Ÿè´£å°†ç‰¹å¾è¿›è¡Œæ˜ å°„åˆ°ä½ å¯¹åº”çš„ä»»åŠ¡ä¸Šï¼Œå¦‚æœæ˜¯æ£€æµ‹ä»»åŠ¡ï¼Œå¯¹åº”çš„å°±æ˜¯4ä¸ªè¾¹ç•Œæ¡†çš„å€¼ä»¥åŠ1ä¸ªç½®ä¿¡åº¦çš„å€¼å’Œä¸€ä¸ªç‰©ä½“ç±»åˆ«çš„å€¼ã€‚å¦‚ä¸‹æ‰€ç¤ºã€‚

  ```python
  # Ultralytics YOLO ğŸš€, AGPL-3.0 license
  """Model head modules."""
  
  import copy
  import math
  
  import torch
  import torch.nn as nn
  from torch.nn.init import constant_, xavier_uniform_
  
  from ultralytics.utils.tal import TORCH_1_10, dist2bbox, dist2rbox, make_anchors
  
  from .block import DFL, BNContrastiveHead, ContrastiveHead, Proto
  from .conv import Conv, DWConv
  from .transformer import MLP, DeformableTransformerDecoder, DeformableTransformerDecoderLayer
  from .utils import bias_init_with_prob, linear_init
  
  __all__ = "Detect", "Segment", "Pose", "Classify", "OBB", "RTDETRDecoder", "v10Detect"
  
  
  ```

åŸºäºä¸Šé¢çš„è®¾è®¡ï¼Œyolo11è¡ç”Ÿå‡ºäº†å¤šç§å˜ç§ï¼Œå¦‚ä¸‹è¡¨æ‰€ç¤ºã€‚ä»–ä»¬å¯ä»¥æ”¯æŒä¸åŒçš„ä»»åŠ¡å’Œä¸åŒçš„æ¨¡å‹å¤§å°ï¼Œåœ¨æœ¬æ¬¡çš„æ•™å­¦ä¸­ï¼Œæˆ‘ä»¬ä¸»è¦å›´ç»•æ£€æµ‹è¿›è¡Œè®²è§£ï¼Œåç»­çš„è¿‡ç¨‹ä¸­ï¼Œè¿˜ä¼šå¯¹åˆ†å‰²ã€å§¿æ€ä¼°è®¡ç­‰ä»»åŠ¡è¿›è¡Œè®²è§£ã€‚

![image-20241024173356022](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20241024173356022.png)

YOLOv11ä»£è¡¨äº†CVé¢†åŸŸçš„é‡å¤§è¿›æ­¥ï¼Œæä¾›äº†å¢å¼ºæ€§èƒ½å’Œå¤šåŠŸèƒ½æ€§çš„å¼•äººæ³¨ç›®çš„ç»„åˆã€‚YOLOæ¶æ„çš„æœ€æ–°è¿­ä»£åœ¨ç²¾åº¦å’Œå¤„ç†é€Ÿåº¦æ–¹é¢æœ‰äº†æ˜¾è‘—çš„æ”¹è¿›ï¼ŒåŒæ—¶å‡å°‘äº†æ‰€éœ€å‚æ•°çš„æ•°é‡ã€‚è¿™æ ·çš„ä¼˜åŒ–ä½¿å¾—YOLOv11ç‰¹åˆ«é€‚åˆå¹¿æ³›çš„åº”ç”¨ç¨‹åºï¼Œä»è¾¹ç¼˜è®¡ç®—åˆ°åŸºäºäº‘çš„åˆ†æã€‚è¯¥æ¨¡å‹å¯¹å„ç§ä»»åŠ¡çš„é€‚åº”æ€§ï¼ŒåŒ…æ‹¬å¯¹è±¡æ£€æµ‹ã€å®ä¾‹åˆ†å‰²å’Œå§¿æ€ä¼°è®¡ï¼Œä½¿å…¶æˆä¸ºå„ç§è¡Œä¸š(å¦‚æƒ…æ„Ÿæ£€æµ‹ã€åŒ»ç–—ä¿å¥å’Œå„ç§å…¶ä»–è¡Œä¸š)çš„æœ‰ä»·å€¼çš„å·¥å…·ã€‚å®ƒçš„æ— ç¼é›†æˆèƒ½åŠ›å’Œæé«˜çš„æ•ˆç‡ä½¿å…¶æˆä¸ºå¯»æ±‚å®æ–½æˆ–å‡çº§å…¶CVç³»ç»Ÿçš„ä¼ä¸šçš„ä¸€ä¸ªæœ‰å¸å¼•åŠ›çš„é€‰æ‹©ã€‚æ€»ä¹‹ï¼ŒYOLOv11å¢å¼ºçš„ç‰¹å¾æå–ã€ä¼˜åŒ–çš„æ€§èƒ½å’Œå¹¿æ³›çš„ä»»åŠ¡æ”¯æŒä½¿å…¶æˆä¸ºè§£å†³ç ”ç©¶å’Œå®é™…åº”ç”¨ä¸­å¤æ‚è§†è§‰è¯†åˆ«æŒ‘æˆ˜çš„å¼ºå¤§è§£å†³æ–¹æ¡ˆã€‚

### å®éªŒç»“æœåˆ†æ

#### æ•°æ®é›†ä»‹ç»

æœ¬æ¬¡æˆ‘ä»¬æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†ä¸ºè”æç—…è™«å®³æ•°æ®é›†ï¼Œæ•°æ®é›†çš„å›¾åƒæ•°é‡ä¸º300, ä¸‹é¢æ˜¯æ•°æ®é›†çš„åˆ†å¸ƒã€‚

![labels](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271507311.jpg)

æˆ‘åœ¨è¿™é‡Œå·²ç»å°†æ•°æ®æŒ‰ç…§yoloåˆ†å‰²æ•°æ®é›†æ ¼å¼è¿›è¡Œäº†å¤„ç†ï¼Œå¤§å®¶åªéœ€è¦åœ¨é…ç½®æ–‡ä»¶ç§å¯¹æœ¬åœ°çš„æ•°æ®åœ°å€è¿›è¡Œé…ç½®å³å¯ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

```yaml
# Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]
path: G:/Upppppdate/0000-48/litchi_yolo_format/ # dataset root dir
train: images/train # train images (relative to 'path') 1052 images
val: images/test # val images (relative to 'path') 225 images
test: images/test # test images (relative to 'path') 227 images
# Classes
names: ['Anthrax', 'Felt_disease', 'Phytophthora_downy', 'Lychee_Tsubaki_elephant', 'Pedicle_borer', 'Turtle_backed_beetle']
```

ä¸‹é¢æ˜¯æ•°æ®é›†çš„éƒ¨åˆ†ç¤ºä¾‹ã€‚

![train_batch1171](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271506855.jpg)

![train_batch1170](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271506491.jpg)

#### å®éªŒç»“æœåˆ†æ

å®éªŒç»“æœçš„æŒ‡æ ‡å›¾å‡ä¿å­˜åœ¨runsç›®å½•ä¸‹ï¼Œ å¤§å®¶åªéœ€è¦å¯¹å®éªŒè¿‡ç¨‹å’ŒæŒ‡æ ‡å›¾çš„ç»“æœè¿›è¡Œè§£æå³å¯ã€‚

å¦‚æœåªæŒ‡æ ‡å›¾çš„å®šä¹‰ä¸æ¸…æ™°ï¼Œè¯·çœ‹è¿™ä¸ªä½ç½®ï¼š[YOLO11æ¨¡å‹æŒ‡æ ‡è§£è¯»-mAPã€Precisionã€Recall_yolo11æ¨¡å‹è®­ç»ƒç‰¹å¾å›¾-CSDNåšå®¢](https://blog.csdn.net/ECHOSON/article/details/144097341)

![results-crack](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/results-crack.png)

train/box_lossï¼ˆè®­ç»ƒé›†çš„è¾¹ç•Œæ¡†æŸå¤±ï¼‰ï¼šéšç€è®­ç»ƒè½®æ¬¡çš„å¢åŠ ï¼Œè¾¹ç•Œæ¡†æŸå¤±é€æ¸é™ä½ï¼Œè¡¨æ˜æ¨¡å‹åœ¨å­¦ä¹ æ›´å‡†ç¡®åœ°å®šä½ç›®æ ‡ã€‚
train/cls_lossï¼ˆè®­ç»ƒé›†çš„åˆ†ç±»æŸå¤±ï¼‰ï¼šåˆ†ç±»æŸå¤±åœ¨åˆæœŸè¿…é€Ÿä¸‹é™ï¼Œç„¶åè¶‹äºå¹³ç¨³ï¼Œè¯´æ˜æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸æé«˜äº†å¯¹æµ·åº•ç”Ÿç‰©çš„åˆ†ç±»å‡†ç¡®æ€§ã€‚
train/dfl_lossï¼ˆè®­ç»ƒé›†çš„åˆ†å¸ƒå¼ç„¦ç‚¹æŸå¤±ï¼‰ï¼šè¯¥æŸå¤±åŒæ ·å‘ˆç°ä¸‹é™è¶‹åŠ¿ï¼Œè¡¨æ˜æ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¼˜åŒ–äº†é¢„æµ‹æ¡†ä¸çœŸå®æ¡†ä¹‹é—´çš„åŒ¹é…ã€‚
metrics/precision(B)ï¼ˆç²¾ç¡®åº¦ï¼‰ï¼šç²¾ç¡®åº¦éšç€è®­ç»ƒè½®æ¬¡çš„å¢åŠ è€Œæé«˜ï¼Œè¯´æ˜æ¨¡å‹åœ¨å‡å°‘è¯¯æŠ¥æ–¹é¢è¡¨ç°è¶Šæ¥è¶Šå¥½ã€‚
metrics/recall(B)ï¼ˆå¬å›ç‡ï¼‰ï¼šå¬å›ç‡ä¹Ÿåœ¨é€æ¸ä¸Šå‡ï¼Œè¡¨æ˜æ¨¡å‹èƒ½å¤Ÿè¯†åˆ«å‡ºæ›´å¤šçš„çœŸå®æµ·åº•ç”Ÿç‰©ã€‚
val/box_lossï¼ˆéªŒè¯é›†çš„è¾¹ç•Œæ¡†æŸå¤±ï¼‰ï¼šéªŒè¯é›†çš„è¾¹ç•Œæ¡†æŸå¤±åŒæ ·ä¸‹é™ï¼Œä½†å¯èƒ½å­˜åœ¨ä¸€äº›æ³¢åŠ¨ï¼Œè¿™å¯èƒ½æ˜¯ç”±äºéªŒè¯é›†çš„å¤šæ ·æ€§æˆ–è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚
val/cls_lossï¼ˆéªŒè¯é›†çš„åˆ†ç±»æŸå¤±ï¼‰ï¼šéªŒè¯é›†çš„åˆ†ç±»æŸå¤±ä¸‹é™è¶‹åŠ¿ä¸è®­ç»ƒé›†ç›¸ä¼¼ï¼Œä½†å¯èƒ½åœ¨æŸäº›ç‚¹ä¸Šå‡ºç°æ³¢åŠ¨ã€‚
val/dfl_lossï¼ˆéªŒè¯é›†çš„åˆ†å¸ƒå¼ç„¦ç‚¹æŸå¤±ï¼‰ï¼šéªŒè¯é›†çš„åˆ†å¸ƒå¼ç„¦ç‚¹æŸå¤±ä¹Ÿåœ¨ä¸‹é™ï¼Œä½†å¯èƒ½å­˜åœ¨ä¸€äº›æ³¢åŠ¨ï¼Œè¿™éœ€è¦è¿›ä¸€æ­¥è§‚å¯Ÿä»¥ç¡®å®šæ˜¯å¦æ˜¯è¿‡æ‹Ÿåˆçš„è¿¹è±¡ã€‚
metrics/mAP50(B)ï¼ˆåœ¨IoUé˜ˆå€¼ä¸º0.5æ—¶çš„å¹³å‡ç²¾åº¦ï¼‰ï¼šmAP50éšç€è®­ç»ƒè½®æ¬¡çš„å¢åŠ è€Œæé«˜ï¼Œè¡¨æ˜æ¨¡å‹åœ¨æ£€æµ‹ä»»åŠ¡ä¸Šçš„æ•´ä½“æ€§èƒ½åœ¨æå‡ã€‚
metrics/mAP50-95(B)ï¼ˆåœ¨IoUé˜ˆå€¼ä»0.5åˆ°0.95çš„å¹³å‡ç²¾åº¦ï¼‰ï¼šmAP50-95çš„æé«˜è¡¨æ˜æ¨¡å‹åœ¨ä¸åŒIoUé˜ˆå€¼ä¸‹çš„æ€§èƒ½éƒ½åœ¨æå‡ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´ä¸¥æ ¼çš„æ€§èƒ½æŒ‡æ ‡ã€‚

![PR_curve](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271506707.png)

å½“ioué˜ˆå€¼ä¸º0.5çš„æ—¶å€™ï¼Œæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„mapå¯ä»¥è¾¾åˆ°å³ä¸Šè§’æ‰€ç¤ºçš„æ•°å€¼ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªé¢„æµ‹å›¾åƒï¼Œå¯ä»¥çœ‹å‡ºï¼Œæˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥æœ‰æ•ˆçš„é¢„æµ‹å‡ºè¿™äº›å°ºåº¦æ¯”è¾ƒå¤šå˜çš„ç›®æ ‡ã€‚

![val_batch0_pred](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271506311.jpg)

![val_batch1_pred](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/202503271506709.jpg)

### ç»“è®º

æœ¬ç ”ç©¶åˆ©ç”¨YOLOv12æ¨¡å‹å¯¹å¸¸è§çš„è”æç—…è™«å®³è¿›è¡Œæ£€æµ‹ï¼Œå–å¾—äº†æ˜¾è‘—çš„ç ”ç©¶æˆæœã€‚è”æä½œä¸ºæˆ‘å›½é‡è¦çš„ç»æµä½œç‰©ï¼Œå…¶ç—…è™«å®³çš„ç²¾å‡†æ£€æµ‹å¯¹äºä¿éšœäº§é‡å’Œå“è´¨å…·æœ‰é‡è¦æ„ä¹‰ã€‚ä¼ ç»Ÿçš„äººå·¥æ£€æµ‹æ–¹æ³•å­˜åœ¨æ•ˆç‡ä½ã€æˆæœ¬é«˜ã€ä¸»è§‚æ€§å¼ºç­‰é—®é¢˜ï¼Œéš¾ä»¥æ»¡è¶³ç°ä»£å†œä¸šçš„éœ€æ±‚ã€‚è¿‘å¹´æ¥ï¼Œæ·±åº¦å­¦ä¹ æŠ€æœ¯å°¤å…¶æ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å›¾åƒè¯†åˆ«é¢†åŸŸå–å¾—äº†å·¨å¤§çªç ´ï¼Œä¸ºè”æç—…è™«å®³çš„è‡ªåŠ¨åŒ–æ£€æµ‹æä¾›äº†æ–°çš„è§£å†³æ–¹æ¡ˆã€‚

åœ¨ä¼—å¤šæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼ŒYOLOç³»åˆ—å› å…¶æ£€æµ‹é€Ÿåº¦å¿«ã€å‡†ç¡®ç‡é«˜è€Œå¤‡å—å…³æ³¨ã€‚æœ¬ç ”ç©¶åŸºäºYOLOv12æ¨¡å‹ï¼Œé’ˆå¯¹è”æç—…è™«å®³å›¾åƒçš„ç‰¹ç‚¹è¿›è¡Œäº†ä¼˜åŒ–ã€‚é€šè¿‡æ„å»ºé«˜è´¨é‡çš„æ•°æ®é›†å¹¶é‡‡ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯ï¼Œæ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°é€‚åº”å¤æ‚è‡ªç„¶ç¯å¢ƒä¸‹çš„å›¾åƒç‰¹å¾ã€‚æ­¤å¤–ï¼Œå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶å’Œè½»é‡åŒ–ç½‘ç»œç»“æ„ï¼Œè¿›ä¸€æ­¥æé«˜äº†æ¨¡å‹çš„æ£€æµ‹ç²¾åº¦å’Œè¿è¡Œæ•ˆç‡ã€‚

å®éªŒç»“æœè¡¨æ˜ï¼ŒYOLOv12æ¨¡å‹åœ¨è”æç—…è™«å®³æ£€æµ‹ä¸­è¡¨ç°å‡ºäº†è¾ƒé«˜çš„å‡†ç¡®ç‡å’Œé²æ£’æ€§ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¯¥æ¨¡å‹èƒ½å¤Ÿå¿«é€Ÿã€å‡†ç¡®åœ°è¯†åˆ«å¤šç§ç—…è™«å®³ç±»å‹ï¼Œä¸ºå®æ—¶ç›‘æµ‹å’Œç²¾å‡†é˜²æ²»æä¾›äº†æœ‰åŠ›æ”¯æŒã€‚åŒæ—¶ï¼Œæ¨¡å‹çš„è½»é‡åŒ–è®¾è®¡ä½¿å…¶é€‚åˆåœ¨ç§»åŠ¨ç»ˆç«¯å’ŒåµŒå…¥å¼è®¾å¤‡ä¸Šéƒ¨ç½²ï¼Œå…·æœ‰å¹¿æ³›çš„åº”ç”¨å‰æ™¯ã€‚

æœªæ¥çš„ç ”ç©¶å¯ä»¥è¿›ä¸€æ­¥ä¼˜åŒ–æ¨¡å‹ç»“æ„ï¼Œæé«˜å¯¹å°ç›®æ ‡å’Œå¤æ‚èƒŒæ™¯çš„æ£€æµ‹èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œç»“åˆå¤šæºæ•°æ®å’Œå¤šæ¨¡å‹èåˆæŠ€æœ¯ï¼Œæœ‰æœ›è¿›ä¸€æ­¥æå‡æ£€æµ‹çš„å‡†ç¡®ç‡å’Œå¯é æ€§ã€‚æ€»ä¹‹ï¼ŒåŸºäºYOLOv12çš„è”æç—…è™«å®³æ£€æµ‹æ¨¡å‹ä¸ºå†œä¸šç—…è™«å®³çš„æ™ºèƒ½åŒ–ç›‘æµ‹æä¾›äº†æ–°çš„æ€è·¯å’Œæ–¹æ³•ï¼Œå…·æœ‰é‡è¦çš„ç†è®ºå’Œå®è·µä»·å€¼ã€‚

### å‚è€ƒæ–‡çŒ®

[1]  Zhang Y , Li H , Bu R ,et al.Fuzzy Multi-objective Requirements for NRP Based on Particle Swarm Optimization[C]//2020.DOI:10.1007/978-3-030-57881-7_13.

[2]  Zhao N , Cao M , Song C ,et al.Trusted Component Decomposition Based on OR-Transition Colored Petri Net[C]//International Conference on Artificial Intelligence and Security.Springer, Cham, 2019.DOI:10.1007/978-3-030-24268-8_41.
DOI: 10.1109/ACCESS.2020.2973568

[3] Song C, Chang H. RST R-CNN: a triplet matching few-shot remote sensing object detection framework[C]//Fourth International Conference on Computer Vision, Application, and Algorithm (CVAA 2024). SPIE, 2025, 13486: 553-568.

[4]   Zhou Q , Yu C . Point RCNN: An Angle-Free Framework for Rotated Object Detection[J]. Remote Sensing, 2022, 14.

[5]  Zhang, Y., Li, H., Bu, R., Song, C., Li, T., Kang, Y., & Chen, T. (2020). Fuzzy Multi-objective Requirements for NRP Based on Particle Swarm Optimization. *International Conference on Adaptive and Intelligent Systems*.

[6]   Li X , Deng J , Fang Y . Few-Shot Object Detection on Remote Sensing Images[J]. IEEE Transactions on Geoscience and Remote Sensing, 2021(99).

[7]   Su W, Zhu X, Tao C, et al. Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information[J]. arXiv preprint arXiv:2211.09807, 2022.

[8]   Chen Q, Wang J, Han C, et al. Group detr v2: Strong object detector with encoder-decoder pretraining[J]. arXiv preprint arXiv:2211.03594, 2022.

[9]   Liu, Shilong, et al. "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection." arXiv preprint arXiv:2303.05499 (2023).

[10] Redmon J, Divvala S, Girshick R, et al. You only look once: Unified, real-time object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 779-788.

[11] Redmon J, Farhadi A. YOLO9000: better, faster, stronger[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7263-7271.

[12] Redmon J, Farhadi A. Yolov3: An incremental improvement[J]. arXiv preprint arXiv:1804.02767, 2018.

[13] Tian Z, Shen C, Chen H, et al. Fcos: Fully convolutional one-stage object detection[C]//Proceedings of the IEEE/CVF international conference on computer vision. 2019: 9627-9636.

[14] Chen L C, Zhu Y, Papandreou G, et al. Encoder-decoder with atrous separable convolution for semantic image segmentation[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 801-818.

[15] Liu W, Anguelov D, Erhan D, et al. Ssd: Single shot multibox detector[C]//Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11â€“14, 2016, Proceedings, Part I 14. Springer International Publishing, 2016: 21-37.

[16] Lin T Y, DollÃ¡r P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2117-2125.

[17] Cai Z, Vasconcelos N. Cascade r-cnn: Delving into high quality object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 6154-6162.

[18] Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[J]. Advances in neural information processing systems, 2015, 28.

[19] Wang R, Shivanna R, Cheng D, et al. Dcn v2: Improved deep & cross network and practical lessons for web-scale learning to rank systems[C]//Proceedings of the web conference 2021. 2021: 1785-1797.

[20] Chen L C, Papandreou G, Schroff F, et al. Rethinking atrous convolution for semantic image segmentation[J]. arXiv preprint arXiv:1706.05587, 2017.

---------------------------------------------------------------------------------------------------------------------

### æ¨¡å‹æ”¹è¿›çš„åŸºæœ¬æµç¨‹ï¼ˆé€‰çœ‹ï¼‰

é¦–å…ˆæˆ‘ä»¬è¯´è¯´å¦‚ä½•åœ¨yoloçš„åŸºç¡€æ¨¡å‹ä¸Šè¿›è¡Œæ”¹è¿›ã€‚

1. åœ¨`block.py`æˆ–è€…`conv.py`ä¸­æ·»åŠ ä½ è¦ä¿®æ”¹çš„æ¨¡å—ï¼Œæ¯”å¦‚æˆ‘åœ¨è¿™é‡Œæ·»åŠ äº†seçš„ç±»ï¼ŒåŒ…å«äº†è¾“å…¥å’Œè¾“å‡ºçš„é€šé“æ•°ã€‚

   ![image-20250108112113879](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250108112113879.png)

   ![image-20250108112249665](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250108112249665.png)

2. åœ¨`init.py`æ–‡ä»¶ä¸­å¼•ç”¨ã€‚

   ![image-20250108112346046](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250108112346046.png)

3. åœ¨`task.py`æ–‡ä»¶ä¸­å¼•ç”¨ã€‚

   ![image-20250108112439566](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250108112439566.png)

4. æ–°å¢é…ç½®æ–‡ä»¶

   ![image-20250108112724144](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250108112724144.png)

### æ¨¡å‹æ”¹è¿›ï¼ˆé€‰çœ‹ï¼‰

æœ¬æ¬¡çš„ç»™å¤§å®¶æä¾›å¥½çš„æ¨¡å‹æ”¹è¿›ä¸»è¦å›´ç»•ä¸¤ä¸ªæ–¹é¢å±•å¼€ï¼Œä¸€ä¸ªæ–¹é¢æ˜¯é€šè¿‡æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶å¢åŠ æ¨¡å‹çš„ç²¾åº¦ï¼Œä¸€ä¸ªæ–¹é¢æ˜¯é€šè¿‡å¼•å…¥ä¸€äº›è½»é‡åŒ–çš„å·ç§¯æ¨¡å—é™ä½æ¨¡å‹çš„è®¡ç®—é‡ã€‚æ³¨æ„ï¼Œå½“ä½ çš„æ¨¡å‹è¿›è¡Œæ”¹å˜ä¹‹åï¼Œè¿™ä¸ªæ—¶å€™ä½ å†ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ•ˆæœä¸ä¼šæ¯”ä½ çš„åŸå§‹é…ç½®æ–‡ä»¶è¦å¥½ï¼Œ å› ä¸ºä½ çš„æ¨¡å‹ç»“æ„å·²ç»æ”¹å˜ï¼Œå†æ¬¡ä½¿ç”¨åŸå§‹çš„cocoçš„é¢„è®­ç»ƒæƒé‡æ¨¡å‹éœ€è¦è€—è´¹æ¯”è¾ƒé•¿çš„æ—¶é—´æ¥çº æ­£ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬è¿›è¡Œå¯¹æ¯”å®éªŒçš„æ—¶å€™è¦ç»Ÿä¸€éƒ½ä¸ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚æˆ–è€…è¯´ä½ å¯ä»¥å…ˆåœ¨cocoæ•°æ®é›†ä¸Šå¯¹ä½ çš„æ”¹è¿›æ¨¡å‹è¿›è¡Œç¬¬ä¸€ä¸ªé˜¶æ®µçš„è®­ç»ƒï¼Œç„¶ååŸºäºç¬¬ä¸€ä¸ªé˜¶æ®µè®­ç»ƒå¥½çš„æƒé‡è¿›è¡Œè¿ç§»å­¦ä¹ ã€‚åè€…çš„æ–¹å¼ä»£ä»·è¾ƒå¤§ï¼Œéœ€è¦ä½ æœ‰è¶³å¤Ÿçš„å¡æ¥åšï¼Œå¯¹äºæˆ‘ä»¬å¹³æ°‘ç©å®¶è€Œè¨€ï¼Œè¿›è¡Œç¬¬äºŒç§å°±è›®å¥½ã€‚

* å‡†ç¡®ç‡æ–¹é¢çš„æ”¹è¿›

  å‡†ç¡®ç‡æ–¹é¢æ”¹è¿›2-CBAM: Convolutional Block Attention Module

  è®ºæ–‡åœ°å€ï¼š[[1807.06521\] CBAM: Convolutional Block Attention Module](https://arxiv.org/abs/1807.06521)

  ![image-20250111194812619](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250111194812619.png)

  CBAMï¼ˆConvolutional Block Attention Moduleï¼‰æ˜¯ä¸€ç§è½»é‡çº§ã€å¯æ‰©å±•çš„æ³¨æ„åŠ›æœºåˆ¶æ¨¡å—ï¼Œé¦–æ¬¡æå‡ºäºè®ºæ–‡ã€ŠCBAM: Convolutional Block Attention Moduleã€‹ï¼ˆECCV 2018ï¼‰ã€‚CBAM åœ¨é€šé“æ³¨æ„åŠ›ï¼ˆChannel Attentionï¼‰å’Œç©ºé—´æ³¨æ„åŠ›ï¼ˆSpatial Attentionï¼‰ä¹‹é—´å¼•å…¥äº†æ¨¡å—åŒ–çš„è®¾è®¡ï¼Œå…è®¸æ¨¡å‹æ›´å¥½åœ°å…³æ³¨é‡è¦çš„ç‰¹å¾é€šé“å’Œä½ç½®ã€‚

  CBAM ç”±ä¸¤ä¸ªæ¨¡å—ç»„æˆï¼š

  **é€šé“æ³¨æ„åŠ›æ¨¡å— (Channel Attention Module)**: å­¦ä¹ æ¯ä¸ªé€šé“çš„é‡è¦æ€§æƒé‡ï¼Œé€šè¿‡åŠ æƒå¢å¼ºé‡è¦é€šé“çš„ç‰¹å¾ã€‚

  **ç©ºé—´æ³¨æ„åŠ›æ¨¡å— (Spatial Attention Module)**: å­¦ä¹ ç©ºé—´ä½ç½®çš„é‡è¦æ€§æƒé‡ï¼Œé€šè¿‡åŠ æƒå…³æ³¨å…³é”®ä½ç½®çš„ç‰¹å¾ã€‚

  è¯¥æ¨¡å—çš„ä»£ç å®ç°å¦‚ä¸‹ï¼š

  ```python
  import torch
  import torch.nn as nn
  
  class ChannelAttention(nn.Module):
      def __init__(self, in_channels, reduction=16):
          """
          é€šé“æ³¨æ„åŠ›æ¨¡å—
          Args:
              in_channels (int): è¾“å…¥é€šé“æ•°
              reduction (int): ç¼©å‡æ¯”ä¾‹å› å­
          """
          super(ChannelAttention, self).__init__()
          self.avg_pool = nn.AdaptiveAvgPool2d(1)  # å…¨å±€å¹³å‡æ± åŒ–
          self.max_pool = nn.AdaptiveMaxPool2d(1)  # å…¨å±€æœ€å¤§æ± åŒ–
  
          self.fc = nn.Sequential(
              nn.Linear(in_channels, in_channels // reduction, bias=False),
              nn.ReLU(inplace=True),
              nn.Linear(in_channels // reduction, in_channels, bias=False)
          )
          self.sigmoid = nn.Sigmoid()
  
      def forward(self, x):
          batch, channels, _, _ = x.size()
  
          # å…¨å±€å¹³å‡æ± åŒ–
          avg_out = self.fc(self.avg_pool(x).view(batch, channels))
          # å…¨å±€æœ€å¤§æ± åŒ–
          max_out = self.fc(self.max_pool(x).view(batch, channels))
  
          # åŠ å’Œåé€šè¿‡ Sigmoid
          out = avg_out + max_out
          out = self.sigmoid(out).view(batch, channels, 1, 1)
  
          # é€šé“åŠ æƒ
          return x * out
  
  
  class SpatialAttention(nn.Module):
      def __init__(self, kernel_size=7):
          """
          ç©ºé—´æ³¨æ„åŠ›æ¨¡å—
          Args:
              kernel_size (int): å·ç§¯æ ¸å¤§å°
          """
          super(SpatialAttention, self).__init__()
          self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)
          self.sigmoid = nn.Sigmoid()
  
      def forward(self, x):
          # é€šé“ç»´åº¦æ±‚å¹³å‡å’Œæœ€å¤§å€¼
          avg_out = torch.mean(x, dim=1, keepdim=True)
          max_out, _ = torch.max(x, dim=1, keepdim=True)
          combined = torch.cat([avg_out, max_out], dim=1)  # æ‹¼æ¥
  
          # å·ç§¯å¤„ç†
          out = self.conv(combined)
          out = self.sigmoid(out)
  
          # ç©ºé—´åŠ æƒ
          return x * out
  
  
  class CBAM(nn.Module):
      def __init__(self, in_channels, reduction=16, kernel_size=7):
          """
          CBAM æ¨¡å—
          Args:
              in_channels (int): è¾“å…¥é€šé“æ•°
              reduction (int): ç¼©å‡æ¯”ä¾‹å› å­
              kernel_size (int): ç©ºé—´æ³¨æ„åŠ›å·ç§¯æ ¸å¤§å°
          """
          super(CBAM, self).__init__()
          self.channel_attention = ChannelAttention(in_channels, reduction)
          self.spatial_attention = SpatialAttention(kernel_size)
  
      def forward(self, x):
          # é€šé“æ³¨æ„åŠ›æ¨¡å—
          x = self.channel_attention(x)
          # ç©ºé—´æ³¨æ„åŠ›æ¨¡å—
          x = self.spatial_attention(x)
          return x
  ```

* é€Ÿåº¦æ–¹é¢çš„æ”¹è¿› 

  é€Ÿåº¦æ–¹é¢æ”¹è¿›2-GhostConv

  **Ghost Convolution** æ˜¯ä¸€ç§è½»é‡åŒ–å·ç§¯æ“ä½œï¼Œé¦–æ¬¡æå‡ºäºè®ºæ–‡ã€ŠGhostNet: More Features from Cheap Operationsã€‹ï¼ˆCVPR 2020ï¼‰ã€‚GhostConv çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨ä¾¿å®œçš„æ“ä½œç”Ÿæˆé¢å¤–çš„ç‰¹å¾å›¾ï¼Œä»¥å‡å°‘è®¡ç®—å¤æ‚åº¦å’Œå‚æ•°é‡ã€‚ã€

  GhostConvçš„æ ¸å¿ƒæ€æƒ³å¦‚æ˜¯ï¼Œå·ç§¯æ“ä½œä¼šç”Ÿæˆå†—ä½™çš„ç‰¹å¾å›¾ã€‚è®¸å¤šç‰¹å¾å›¾ä¹‹é—´å­˜åœ¨é«˜ç›¸å…³æ€§ã€‚GhostConv çš„ç›®æ ‡æ˜¯é€šè¿‡å‡å°‘å†—ä½™ç‰¹å¾å›¾çš„è®¡ç®—æ¥åŠ é€Ÿç½‘ç»œçš„æ¨ç†ã€‚GhostConv çš„ç»“æ„å¦‚ä¸‹ï¼š

  ![image-20250109220155390](https://vehicle4cm.oss-cn-beijing.aliyuncs.com/imgs/image-20250109220155390.png)

  **ä¸»ç‰¹å¾å›¾**: ä½¿ç”¨æ ‡å‡†å·ç§¯ç”Ÿæˆä¸€éƒ¨åˆ†ç‰¹å¾å›¾ã€‚

  **å‰¯ç‰¹å¾å›¾**: ä»ä¸»ç‰¹å¾å›¾ä¸­é€šè¿‡ç®€å•çš„çº¿æ€§æ“ä½œï¼ˆå¦‚æ·±åº¦å·ç§¯ï¼‰ç”Ÿæˆã€‚

  ä»£ç å®ç°å¦‚ä¸‹ï¼š

  ```python
  import torch
  import torch.nn as nn
  
  class GhostConv(nn.Module):
      def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, ratio=2, dw_kernel_size=3):
          """
          Ghost Convolution å®ç°
          Args:
              in_channels (int): è¾“å…¥é€šé“æ•°
              out_channels (int): è¾“å‡ºé€šé“æ•°
              kernel_size (int): å·ç§¯æ ¸å¤§å°
              stride (int): å·ç§¯æ­¥å¹…
              padding (int): å·ç§¯å¡«å……
              ratio (int): å‰¯ç‰¹å¾ä¸ä¸»ç‰¹å¾çš„æ¯”ä¾‹
              dw_kernel_size (int): æ·±åº¦å·ç§¯çš„å·ç§¯æ ¸å¤§å°
          """
          super(GhostConv, self).__init__()
          self.out_channels = out_channels
          self.primary_channels = out_channels // ratio  # ä¸»ç‰¹å¾å›¾é€šé“æ•°
          self.ghost_channels = out_channels - self.primary_channels  # å‰¯ç‰¹å¾å›¾é€šé“æ•°
  
          # ä¸»ç‰¹å¾å›¾çš„æ ‡å‡†å·ç§¯
          self.primary_conv = nn.Conv2d(
              in_channels, self.primary_channels, kernel_size, stride, padding, bias=False
          )
          self.bn1 = nn.BatchNorm2d(self.primary_channels)
  
          # å‰¯ç‰¹å¾å›¾çš„æ·±åº¦å·ç§¯
          self.ghost_conv = nn.Conv2d(
              self.primary_channels, self.ghost_channels, dw_kernel_size, stride=1,
              padding=dw_kernel_size // 2, groups=self.primary_channels, bias=False
          )
          self.bn2 = nn.BatchNorm2d(self.ghost_channels)
  
          self.relu = nn.ReLU(inplace=True)
  
      def forward(self, x):
          # ä¸»ç‰¹å¾å›¾
          primary_features = self.primary_conv(x)
          primary_features = self.bn1(primary_features)
  
          # å‰¯ç‰¹å¾å›¾
          ghost_features = self.ghost_conv(primary_features)
          ghost_features = self.bn2(ghost_features)
  
          # åˆå¹¶ä¸»ç‰¹å¾å›¾å’Œå‰¯ç‰¹å¾å›¾
          output = torch.cat([primary_features, ghost_features], dim=1)
          output = self.relu(output)
  
          return output
  ```

### 
